{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3FKqSUEWj72flMMmBvgrB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayuti/Topic_Modeling/blob/main/Topic_Modeling_spark_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import PySpark and Spark NLP\n",
        "\n"
      ],
      "metadata": {
        "id": "nkuczkP5tymQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "18_CbELFoAd_"
      },
      "outputs": [],
      "source": [
        "# !pip install -q pyspark==3.3.0 spark-nlp==4.2.4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting of Spark Session\n",
        "\n"
      ],
      "metadata": {
        "id": "izyuAOnXtnAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spark = sparknlp.start()\n"
      ],
      "metadata": {
        "id": "aVS1BD2loCTE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload sample CSV file to Colab session storage and read into Spark dataframe\n",
        "\n"
      ],
      "metadata": {
        "id": "jMMDDqSjt_h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"csv\").option(\"header\",True).load(\"CommentsApril2017.csv\")\n",
        "df.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbPgkpcfoCW1",
        "outputId": "c03369f5-a113-438c-91fe-4133cb8f163b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19093"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQvRTC2UoCaM",
        "outputId": "54e3473e-0639-4bee-8389-eb3178c5f7a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- approveDate: string (nullable = true)\n",
            " |-- commentBody: string (nullable = true)\n",
            " |-- commentID: string (nullable = true)\n",
            " |-- commentSequence: string (nullable = true)\n",
            " |-- commentTitle: string (nullable = true)\n",
            " |-- commentType: string (nullable = true)\n",
            " |-- createDate: string (nullable = true)\n",
            " |-- depth: string (nullable = true)\n",
            " |-- editorsSelection: string (nullable = true)\n",
            " |-- parentID: string (nullable = true)\n",
            " |-- parentUserDisplayName: string (nullable = true)\n",
            " |-- permID: string (nullable = true)\n",
            " |-- picURL: string (nullable = true)\n",
            " |-- recommendations: string (nullable = true)\n",
            " |-- recommendedFlag: string (nullable = true)\n",
            " |-- replyCount: string (nullable = true)\n",
            " |-- reportAbuseFlag: string (nullable = true)\n",
            " |-- sharing: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- timespeople: string (nullable = true)\n",
            " |-- trusted: string (nullable = true)\n",
            " |-- updateDate: string (nullable = true)\n",
            " |-- userDisplayName: string (nullable = true)\n",
            " |-- userID: string (nullable = true)\n",
            " |-- userLocation: string (nullable = true)\n",
            " |-- userTitle: string (nullable = true)\n",
            " |-- userURL: string (nullable = true)\n",
            " |-- inReplyTo: string (nullable = true)\n",
            " |-- articleID: string (nullable = true)\n",
            " |-- sectionName: string (nullable = true)\n",
            " |-- newDesk: string (nullable = true)\n",
            " |-- articleWordCount: string (nullable = true)\n",
            " |-- printPage: string (nullable = true)\n",
            " |-- typeOfMaterial: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing Pipeline using Spark NLP\n"
      ],
      "metadata": {
        "id": "CdVG81UWuJpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark NLP requires the input dataframe or column to be converted to document\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"commentBody\") \\\n",
        "    .setOutputCol(\"document\") \\\n",
        "    .setCleanupMode(\"shrink\")\n",
        "\n",
        "# Split sentence into tokens\n",
        "tokenizer = Tokenizer() \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"token\")\n",
        "\n",
        "# Clean unwanted characters and garbage\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\n",
        "\n",
        "# Remove stopwords\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"cleanTokens\"]) \\\n",
        "    .setOutputCols([\"tokens\"]) \\\n",
        "    .setOutputAsArray(True) \\\n",
        "    .setCleanAnnotations(False)\n",
        "\n",
        "# Build pipeline\n",
        "nlp_pipeline = Pipeline(\n",
        "    stages=[document_assembler,\n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner,\n",
        "            finisher])\n",
        "\n",
        "# Train pipeline\n",
        "nlp_model = nlp_pipeline.fit(df)\n",
        "\n",
        "# Apply pipeline to dataframe\n",
        "processed_df  = nlp_model.transform(df)\n",
        "\n",
        "# Get tokens\n",
        "tokens_df = processed_df.select('tokens').limit(10000)\n"
      ],
      "metadata": {
        "id": "dZW2LjtwoCsH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering\n"
      ],
      "metadata": {
        "id": "3p_8cPZ0uOMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=1000, minDF=3.0)\n",
        "\n",
        "# Train the model\n",
        "cv_model = cv.fit(tokens_df)\n",
        "\n",
        "# Transform the data and output features\n",
        "vectorized_tokens = cv_model.transform(tokens_df)\n"
      ],
      "metadata": {
        "id": "K5s7z1s7oCvs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the LDA (Latent Dirichlet Allocation) Model\n"
      ],
      "metadata": {
        "id": "jnsw37eUuUNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "num_topics = 15\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)\n"
      ],
      "metadata": {
        "id": "JARnp54EoZqn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the topics\n",
        "\n"
      ],
      "metadata": {
        "id": "jrZ_OfZ8uV6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# Extract vocabulary from CountVectorizer\n",
        "vocab = cv_model.vocabulary\n",
        "topics = model.describeTopics()\n",
        "topics_rdd = topics.rdd\n",
        "topics_words = topics_rdd \\\n",
        "       .map(lambda row: row['termIndices']) \\\n",
        "       .map(lambda idx_list: [vocab[idx] for idx in idx_list]) \\\n",
        "       .collect()\n",
        "\n",
        "# Add each array of words to a new column\n",
        "def get_words(idx_list):\n",
        "    return [vocab[idx] for idx in idx_list]\n",
        "\n",
        "udf_get_words = udf(get_words, ArrayType(StringType()))\n",
        "topics = topics.withColumn(\"words\", udf_get_words(topics.termIndices))\n",
        "\n",
        "# From topics dataframa select topic and words columns\n",
        "topics_df = topics.select(\"topic\", \"words\")\n",
        "\n",
        "topics_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IrqlKFtoZt-",
        "outputId": "20e6095c-f15f-4bcd-9e43-b64f1cda58db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------------------------------------------------------------+\n",
            "|topic|words                                                                                |\n",
            "+-----+-------------------------------------------------------------------------------------+\n",
            "|0    |[people, news, many, like, women, amp, dont, Fox, one, News]                         |\n",
            "|1    |[Carrie, maybe, Trump, guy, every, know, minds, people, four, name]                  |\n",
            "|2    |[election, Trump, Russians, get, every, time, make, God, Spicer, Sean]               |\n",
            "|3    |[nuclear, Democratic, White, House, think, Party, comments, Thank, book, problem]    |\n",
            "|4    |[people, country, time, different, One, story, America, North, budget, saying]       |\n",
            "|5    |[Bill, OReilly, abuse, father, DJ, knew, family, years, believe, old]                |\n",
            "|6    |[tax, like, Trump, people, money, government, US, American, trade, border]           |\n",
            "|7    |[Trump, people, one, dont, like, get, see, think, much, know]                        |\n",
            "|8    |[like, people, candy, cotton, think, man, looks, selling, cemetery, Vancouver]       |\n",
            "|9    |[Trump, people, women, one, men, years, well, old, even, many]                       |\n",
            "|10   |[human, people, rights, right, must, basic, always, society, becomes, stop]          |\n",
            "|11   |[Trump, Pence, Ryan, GOP, Paul, election, right, many, simple, economy]              |\n",
            "|12   |[man, see, candy, background, cotton, like, stick, people, family, grave]            |\n",
            "|13   |[play, college, game, school, kids, baseball, team, players, games, academic]        |\n",
            "|14   |[brain, standing, near, happening, New, information, happen, surprise, expect, given]|\n",
            "+-----+-------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oBXePq5oZzK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vrDQny_goZ83"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6un__ZwNoaAQ"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}